import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn
import os

os.makedirs('./data', exist_ok=True)

admissions_data = """34.62365962451697,78.0246928153624,030.28671076822607,43.89499752400101,
0
35.84740876993872,
72.90219802708364,
0
60.18259938620976,
86.30855209546826,
1
79.0327360507101,
75.3443764369103,
1
45.08327747668339,
56.3163717815305,
0
61.10666453684766,
96.51142588489624,
1
75.02474556738889,
46.55401354116538,
1
76.09878670226257,
87.42056971926803,
1
84.43281996120035,
43.53339331072109,
1
95.86155507093572,
38.22527805795094,
0
75.01365838958247,
30.60326323428011,
0
82.30705337399482,
76.48196330235604,
1
69.36458875970939,
97.71869196188608,
1
39.53833914367223,
76.03681085115882,
0
53.9710521485623,
89.20735013750205,
1
69.07014406283025,
52.74046973016765,
1
67.94685547711617,
46.67857410673128,
0
70.66150955499435,
92.92713789364831,
1
76.97878372747498,
47.57596364975532,
1
67.37202754570876,
42.83843832029179,
0
89.67677575072079,
65.79936592745237,
1
50.534788289883,
48.85581152764205,
0
34.21206097786789,
44.20952859866288,
0
77.9240914545704,
68.9723599933059,
1
62.27101367004632,
69.95445795447587,
1
80.1901807509566,
44.82162893218353,
1
93.114388797442,
38.80067033713209,
0
61.83020602312595,
50.25610789244621,
0
38.78580379679423,
64.99568095539578,
0
61.379289447425,
72.80788731317097,
1
85.40451939411645,
57.05198397627122,
1
52.10797973193984,
63.12762376881715,
0
52.04540476831827,
69.43286012045222,
1
40.23689373545111,
71.16774802184875,
0
54.63510555424817,
52.21388588061123,
0
33.91550010906887,
98.86943574220611,
0
64.17698887494485,
80.90806058670817,
1
74.78925295941542,
41.57341522824434,
0
34.1836400264419,
75.2377203360134,
0
83.90239366249155,
56.30804621605327,
1
51.54772026906181,
46.85629026349976,
0
94.44336776917852,
65.56892160559052,
1
82.36875375713919,
40.61825515970618,
0
51.04775177128865,
45.82270145776001,
0
62.22267576120188,
52.06099194836679,
0
77.19303492601364,
70.45820000180959,
1
97.77159928000232,
86.7278223300282,
1
62.07306379667647,
96.76882412413983,
1
91.56497449807442,
88.69629254546599,
1
79.94481794066932,
74.16311935043758,
1
99.2725269292572,
60.99903099844988,
1
90.54671411399852,
43.39060180650027,
1
34.52451385320009,
60.39634245837173,
0
50.2864961189907,
49.80453881323059,
0
49.58667721632031,
59.80895099453265,
0
97.64563396007767,
68.86157272420604,
1
32.57720016809309,
95.59854761387875,
0
74.24869136721598,
69.82457122657193,
1
71.79646205863379,
78.45356224515052,
1
75.3956114656803,
85.75993667331619,
1
35.28611281526193,
47.02051394723416,
0
56.25381749711624,
39.26147251058019,
0
30.05882244669796,
49.59297386723685,
0
44.66826172480893,
66.45008614558913,
0
66.56089447242954,
41.09209807936973,
0
40.45755098375164,
97.53518548909936,
1
49.07256321908844,
51.88321182073966,
0
80.27957401466998,
92.11606081344084,
1
66.74671856944039,
60.99139402740988,
1
32.72283304060323,
43.30717306430063,
0
64.0393204150601,
78.03168802018232,
1
72.34649422579923,
96.22759296761404,
1
60.45788573918959,
73.09499809758037,
1
58.84095621726802,
75.85844831279042,
1
99.82785779692128,
72.36925193383885,
1
47.26426910848174,
88.47586499559782,
1
50.45815980285988,
75.80985952982456,
1
60.45555629271532,
42.50840943572217,
0
82.22666157785568,
42.71987853716458,
0
88.9138964166533,
69.80378889835472,
1
94.83450672430196,
45.69430680250754,
1
67.31925746917527,
66.58935317747915,
1
57.23870631569862,
59.51428198012956,
1
80.36675600171273,
90.96014789746954,
1
68.46852178591112,
85.59430710452014,
1
42.0754545384731,
78.84478600148043,
0
75.47770200533905,
90.42453899753964,
1
78.63542434898018,
96.64742716885644,
1
52.34800398794107,
60.76950525602592,
0
94.09433112516793,
77.15910509073893,
1
90.44855097096364,
87.50879176484702,
1
55.48216114069585,
35.57070347228866,
0
74.49269241843041,
84.84513684930135,
1
89.84580670720979,
45.35828361091658,
1
83.48916274498238,
48.38028579728175,
1
42.2617008099817,
87.10385094025457,
1
99.31500880510394,
68.77540947206617,
1
55.34001756003703,
64.9319380069486,
1
74.77589300092767,
89.52981289513276,
1"""

with open('./data/admissions.csv', 'w') as f:
    f.write(admissions_data)

X_features = ['Exam 1 score', 'Exam 2 score'
]
y_name = ['Admitted'
]
data = pd.read_csv("./data/admissions.csv", header=None, names=X_features + y_name)

X = data[X_features
].values
y = data[y_name
].values
m = y.size  # number of training examples

pos = y == 1
neg = y == 0
plt.scatter(X[pos[
        : ,
        0
    ],
    0
], X[pos[
        : ,
        0
    ],
    1
], marker='o', c='k', label='Admitted')
plt.scatter(X[neg[
        : ,
        0
    ],
    0
], X[neg[
        : ,
        0
    ],
    1
], marker='o', c='y', label='Not Admitted')
plt.xlabel('Bidimensional data')
plt.ylabel('Score')
plt.legend()
plt.show()


def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_cost(X, y, w, b):
    m = y.size
    z = np.dot(X, w) + b
    h = sigmoid(z)
    cost = - (1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    return cost

def compute_gradient(X, y, w, b):
    m = X.shape
    z = np.dot(X, w) + b
    h = sigmoid(z)
    error = h - y
    dj_dw = (1 / m) * np.dot(X.T, error)
    dj_db = (1 / m) * np.sum(error)
    return dj_db, dj_dw

def gradient_descent(X, y, w_in, b_in, alpha, num_iters):
    w = w_in.copy()
    b = b_in
    for i in range(num_iters):
        dj_db, dj_dw = compute_gradient(X, y, w, b)
        w -= alpha * dj_dw
        b -= alpha * dj_db
        if i % 100 == 0:
            cost = compute_cost(X, y, w, b)
            print(f"Iteration {i}: Cost {cost}")
    return w, b

def predict(X, w, b):
    z = np.dot(X, w) + b
    h = sigmoid(z)
    return h >= 0.5


w_init = np.zeros((X.shape[
    1
],
1))
b_init = 0.0
alpha = 0.001  # Taxa de aprendizado
num_iters = 10000

y = y.reshape(-1,
1)
cost_history = []
w = w_init.copy()
b = b_init

print(f"\nParâmetros iniciais:")
print(f"  w: {w.flatten()}")
print(f"  b: {b}")
print(f"  Taxa de aprendizado (α): {alpha}")
print(f"  Número de iterações: {num_iters}")


for i in range(num_iters):
    dj_db, dj_dw = compute_gradient(X, y, w, b)
    w -= alpha * dj_dw
    b -= alpha * dj_db
    
    # armazenar custo a cada iteração
    cost = compute_cost(X, y, w, b)
    cost_history.append(cost)
    
    if i % 1000 == 0:
        print(f"Iteração {i:5d}: Custo = {cost:.6f}")

w_final = w
b_final = b

print(f"\nResultado:")
print(f"  w: {w_final.flatten()}")
print(f"  b: {b_final:.6f}")
print(f"  Custo final: {cost_history[-1]:.6f}")

y_pred = predict(X, w_final, b_final)
y_pred_proba = sigmoid(np.dot(X, w_final) + b_final)

accuracy = np.mean(y_pred == y) * 100
print(f"\nAcurácia do modelo: {accuracy:.2f}%")

print(f"\nExemplos de previsões (primeiros 10 casos):")
print(f"{'Exam 1':>10} | {'Exam 2':>10} | {'Real':>6} | {'Predito':>8} | {'Prob.':>8}")
for i in range(10):
    print(f"{X[i, 0]:10.2f} | {X[i, 1]:10.2f} | {int(y[i, 0]):6d} | {int(y_pred[i, 0]):8d} | {y_pred_proba[i, 0]:8.4f}")


fig = plt.figure(figsize=(18,
12))

# Custo
ax1 = plt.subplot(2,
3,
1)
ax1.plot(cost_history, linewidth=2, color='blue')
ax1.set_xlabel('Iteração', fontsize=11)
ax1.set_ylabel('Custo J(w, b)', fontsize=11)
ax1.set_title('Evolução da Função de Custo (Escala Linear)', fontsize=12, fontweight='bold')
ax1.grid(True, alpha=0.3)
ax1.text(0.98,
0.98, f'Custo Final: {cost_history[
        -1
    ]:.6f
}', 
         transform=ax1.transAxes, ha='right', va='top',
         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),
         fontsize=10)

# escala log
ax2 = plt.subplot(2,
3,
2)
ax2.plot(cost_history, linewidth=2, color='green')
ax2.set_xlabel('Iteração', fontsize=11)
ax2.set_ylabel('Custo J(w, b)', fontsize=11)
ax2.set_yscale('log')
ax2.set_title('Evolução da Função de Custo (Escala Log)', fontsize=12, fontweight='bold')
ax2.grid(True, alpha=0.3)

# Fronteira
ax3 = plt.subplot(2,
3,
3)
pos = y == 1
neg = y == 0
ax3.scatter(X[pos[
        : ,
        0
    ],
    0
], X[pos[
        : ,
        0
    ],
    1
], marker='o', c='green', 
            s=80, edgecolors='black', linewidth=1.5, label='Admitido (y=1)', alpha=0.7)
ax3.scatter(X[neg[
        : ,
        0
    ],
    0
], X[neg[
        : ,
        0
    ],
    1
], marker='x', c='red', 
            s=80, linewidth=2, label='Não Admitido (y=0)', alpha=0.7)
x1_min, x1_max = X[
    : ,
    0
].min() - 5, X[
    : ,
    0
].max() + 5
x2_min, x2_max = X[
    : ,
    1
].min() - 5, X[
    : ,
    1
].max() + 5
xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max,
200),
                       np.linspace(x2_min, x2_max,
200))
X_grid = np.c_[xx1.ravel(), xx2.ravel()
]
Z = sigmoid(np.dot(X_grid, w_final) + b_final)
Z = Z.reshape(xx1.shape)

contours = ax3.contour(xx1, xx2, Z, levels=[
    0.5
], colors='blue', linewidths=3)
ax3.clabel(contours, inline=True, fontsize=10, fmt='Decisão: %.1f')
ax3.contourf(xx1, xx2, Z, levels=20, cmap='RdYlGn', alpha=0.3)
ax3.set_xlabel('Nota Exame 1', fontsize=11)
ax3.set_ylabel('Nota Exame 2', fontsize=11)
ax3.set_title('Dados com Fronteira de Decisão', fontsize=12, fontweight='bold')
ax3.legend(loc='lower right')
ax3.grid(True, alpha=0.3)

# Função Sigmoid
ax4 = plt.subplot(2,
3,
4)
z_range = np.linspace(-10,
10,
100)
sigmoid_vals = sigmoid(z_range)
ax4.plot(z_range, sigmoid_vals, linewidth=3, color='purple')
ax4.axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='Limite de Decisão (0.5)')
ax4.axvline(x=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)
ax4.set_xlabel('z = w·x + b', fontsize=11)
ax4.set_ylabel('σ(z)', fontsize=11)
ax4.set_title('Função Sigmoid', fontsize=12, fontweight='bold')
ax4.legend()
ax4.grid(True, alpha=0.3)
ax4.text(0.02,
0.98, 'σ(z) = 1 / (1 + e⁻ᶻ)', 
         transform=ax4.transAxes, ha='left', va='top',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),
         fontsize=10, style='italic')

# Distribuição de Probabilidades
ax5 = plt.subplot(2,
3,
5)
probs_class_0 = y_pred_proba[y.flatten() == 0
]
probs_class_1 = y_pred_proba[y.flatten() == 1
]
ax5.hist(probs_class_0, bins=20, alpha=0.6, label='Classe 0 (Não Admitido)', 
         color='red', edgecolor='black')
ax5.hist(probs_class_1, bins=20, alpha=0.6, label='Classe 1 (Admitido)', 
         color='green', edgecolor='black')
ax5.axvline(x=0.5, color='blue', linestyle='--', linewidth=2, label='Threshold = 0.5')
ax5.set_xlabel('Probabilidade Predita', fontsize=11)
ax5.set_ylabel('Frequência', fontsize=11)
ax5.set_title('Distribuição de Probabilidades por Classe', fontsize=12, fontweight='bold')
ax5.legend()
ax5.grid(True, alpha=0.3, axis='y')

# Matriz de Confusão Visual
ax6 = plt.subplot(2,
3,
6)
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y, y_pred)
im = ax6.imshow(cm, interpolation='nearest', cmap='Blues')
ax6.set_title('Matriz de Confusão', fontsize=12, fontweight='bold')
plt.colorbar(im, ax=ax6)
tick_marks = np.arange(2)
ax6.set_xticks(tick_marks)
ax6.set_yticks(tick_marks)
ax6.set_xticklabels(['Não Admitido', 'Admitido'
])
ax6.set_yticklabels(['Não Admitido', 'Admitido'
])
ax6.set_xlabel('Predito', fontsize=11)
ax6.set_ylabel('Real', fontsize=11)

thresh = cm.max() / 2
for i in range(cm.shape[
    0
]):
    for j in range(cm.shape[
    1
]):
        ax6.text(j, i, format(cm[i, j
], 'd'),
                ha="center", va="center",
                color="white" if cm[i, j
] > thresh else "black",
                fontsize=16, fontweight='bold')

plt.suptitle('ANÁLISE COMPLETA DA REGRESSÃO LOGÍSTICA', 
             fontsize=16, fontweight='bold', y=0.995)
plt.tight_layout()
plt.show()

print(f"  • Acurácia: {accuracy:.2f}%")
print(f"  • Custo inicial: {cost_history[0]:.6f}")
print(f"  • Custo final: {cost_history[-1]:.6f}")
print(f"  • Redução do custo: {((cost_history[0] - cost_history[-1]) / cost_history[0] * 100):.2f}%")
print(f"\n  • Matriz:")
print(f"      VP: {cm[1, 1]} | FP: {cm[0, 1]}")
print(f"      FN: {cm[1, 0]} | VN: {cm[0, 0]}")